{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (EX) News article processing (with ML pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMoMrQ-je41K"
   },
   "outputs": [],
   "source": [
    "# spark setup, only if you need to specify your paths\n",
    "\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Program Files\\Java\\jdk-11.0.2\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\Program Files\\Spark\\spark-3.5.5-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DC97uPhMe5cn"
   },
   "outputs": [],
   "source": [
    "# findspark helps locate the environment variables\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7yr9_JzZwQ6"
   },
   "source": [
    "# `agnews` Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S51T-4qYZ0LU"
   },
   "outputs": [],
   "source": [
    "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews.csv -O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WOEde4yY8_u"
   },
   "source": [
    "# Pipelining with PySpark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rWgYKCtGY8vy"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline # pipeline to transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC9bF4RLQvsn"
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"local[*]\")\n",
    "         .appName(\"AG news\")\n",
    "         .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhcOjvnyqOUf"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5WQzZTcqtSZ"
   },
   "outputs": [],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGqkLxIDt8L8"
   },
   "source": [
    "# Arrange columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jgV-jK9snHr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws, col # to concatinate cols\n",
    "\n",
    "# renaming 'Class Index' col to 'label'\n",
    "df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "# concatenating texts\n",
    "df = df.withColumn('text', concat_ws(\" \", \"Title\", \"Description\"))\n",
    "\n",
    "df = df.select('label', 'text')\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4s8B9lDuSZm"
   },
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iluZMjpnuNnT"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer # tokenizer\n",
    "\n",
    "# convert sentences to list of words\n",
    "tokenizer = RegexTokenizer(inputCol='text', outputCol='words', pattern=\"\\\\W\")\n",
    "\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "df.select(['label', 'text', 'words']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zujVzF9vGQ4"
   },
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwqRSHqtu12B"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "# remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "df = stopwords_remover.transform(df)\n",
    "\n",
    "\n",
    "df.select(['label', 'text', 'words', 'filtered']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lB7I_H3vSKJ"
   },
   "source": [
    "# Term frequency, Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vfAqoeZvRwV"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# calculate term frequency in each article (row)\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "featurized_data = hashing_tf.transform(df)\n",
    "\n",
    "featurized_data.select('raw_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0bYYaoZvihR"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# inverse document frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "idf_vectorizer = idf.fit(featurized_data)\n",
    "\n",
    "rescaled_data = idf_vectorizer.transform(featurized_data)\n",
    "\n",
    "rescaled_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwXEZpVswcO_"
   },
   "outputs": [],
   "source": [
    "rescaled_data.select('raw_features').show(2, truncate=False)\n",
    "rescaled_data.select('features').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "macNK1smxX5s"
   },
   "source": [
    "# Training a multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OeLPS8HwxXXN"
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "(train, test) = rescaled_data.randomSplit([0.75, 0.25], seed=42)\n",
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01Ml9cUlww1m"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family='multinomial',\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpkPVcdnz0Mi"
   },
   "source": [
    "# Prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t880vBmqzswD"
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select('probability', 'prediction').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.withColumn('correctFlag', (col('label') == col('prediction')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "predictions.select(avg(col('correctFlag').cast(FloatType())).alias('acurracy')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaiuTIx53cyq"
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# labels = [\"World\", \"Sports\", \"Business\",\"Science\"]\n",
    "\n",
    "# take only the predictions\n",
    "preds_and_labels = predictions.select(['prediction','label']) \\\n",
    "                              .withColumn('label', col('label') \\\n",
    "                              .cast(FloatType())) \\\n",
    "                              .orderBy('prediction')\n",
    "\n",
    "\n",
    "preds_and_labels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNtcwxpY4REC"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "metrics = MulticlassMetrics(predictionAndLabels=preds_and_labels.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjK_PlwD4fqi"
   },
   "source": [
    "# Pipelining, from start to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYlA_fev4iV4"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = spark.read.csv(\"agnews.csv\", inferSchema=True, header=True)\n",
    "\n",
    "def arrangeColumns(df):\n",
    "  # Renaming 'Class Index' col to 'label'\n",
    "  df = df.withColumnRenamed('Class Index', 'label')\n",
    "\n",
    "  # Add a new column 'text' by joining 'Title' and 'Description'\n",
    "  df = df.withColumn(\"text\", concat_ws(\" \", \"Title\", 'Description'))\n",
    "\n",
    "  # Select new text feature and labels\n",
    "  df = df.select('label', 'text')\n",
    "  return df\n",
    "\n",
    "df = arrangeColumns(df)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# term frequency\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\",\n",
    "                       outputCol=\"raw_features\",\n",
    "                       numFeatures=16384)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# model\n",
    "lr = LogisticRegression(featuresCol='features',\n",
    "                        labelCol='label',\n",
    "                        family=\"multinomial\",\n",
    "                        regParam=0.3,\n",
    "                        elasticNetParam=0,\n",
    "                        maxIter=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9_LHoAR5Cc-"
   },
   "outputs": [],
   "source": [
    "# Put everything in pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer,\n",
    "                            stopwords_remover,\n",
    "                            hashing_tf,\n",
    "                            idf,\n",
    "                            lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "\n",
    "# transform and train\n",
    "dataset = pipelineFit.transform(df)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOAVBCKK/HPnCbpIGlAzRAR",
   "include_colab_link": true,
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
